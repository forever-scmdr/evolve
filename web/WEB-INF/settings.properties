#######################################
## Different file paths
#######################################

# D:\PROJECTS\HI_TECH\oremonte\.metadata\.plugins\org.eclipse.wst.server.core\tmp1\wtpwebapps\Ecommander\WEB-INF\

# path to site XML files
paths.rel_base_folder = WEB-INF/ec_xml/

# data model file…
paths.data_model = model.xml

# custom data model file (items created by user)
paths.data_model_custom = model_custom.xml

# page models file
paths.pages_model = pages.xml

# domains file
paths.domains = domains.xml

# users and user groups file
paths.users = users.xml

# URL conversion file (not used now)
paths.urls = WEB-INF/url.txt

# all web application files root directory (if not set it is created automatically - this is recommended option)
paths.context_root = C:/PROJECTS/V8_EVOLVE/mystery/web/

# root directory for uploaded files
paths.files_folder = files/

# directory for XSL styles files
paths.styles_folder = WEB-INF/xsl_main/

# directory for cached HTML files (ready for sending to users)
paths.cache_folder = WEB-INF/_cache/

# directory for cached XML fragments that are used to create XML version of pages before XSLT transformation
paths.cache_xml_folder = WEB-INF/_cachexml/

# Lucene index directory
paths.lucene_index_folder = WEB-INF/_lucene/

#######################################
## Other settings
#######################################

# Is pages cache enabled (disable only for debugging purposes)
settings.enable_cache = yes

# welcome page name (in pages.xml file)
url.welcome_page = index

# client-server protocol scheme (http or https)
url.scheme = http
url.https.test.header = x-forwarded-proto
url.https.test.value = https

#######################################
## LocaleŒ
#######################################

# language
locale.language=ru

# country
locale.country=BE

#######################################
## Code generation
#######################################

# Class name for holding item and parameter names 
generated.constants_class = ItemNames

#######################################
## Web crawling (parsing)
#######################################

# crawler4j storage dir (where temporary files are stored)
parsing.storage_dir = crawler4j
# number of parallel crawling threads
parsing.number_of_crawlers = 1
# number of milliseconds between requests
parsing.politeness = 1000
# maximum number of pages to fetch
parsing.max_pages = -1
# maximum crawling depth
parsing.max_depth = -1
# file with proxy list (ip addresses)
parsing.proxies_file = WEB-INF/_crawler/conf/proxies.txt
# after how many pages loaded through current proxy we need to change proxy (0 - never change proxy)
parsing.urls_per_proxy = 0
# file with seed urls and urls whose pages should be parsed and transformed with corresponding XSL templates
# file format:
# regex_url_mask <space> template <space> http://sample.url/to_test_regex
# or
# seed_url <empty string>
# or
# url_to_be_processed_to_find_links_but_not_parsed <space> - <dash> <space> http://sample.url/to_test_regex
# # - comment symbol. Strings beginning with # are comments
parsing.urls = WEB-INF/_crawler/conf/urls.txt
# XSL files directory (style files that are referenced in "urls" file)
parsing.styles_dir = WEB-INF/_crawler/conf/styles/
# the name of the directory where results are stored
# WEB-INF/_crawler/parse/result/
parsing.result_dir = G:/parse/result/
# the name of the resulting XML file
parsing.result_file = WEB-INF/_crawler/parse/result/result.xml