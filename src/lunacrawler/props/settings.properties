# run mode: just download source files (get), just parse source files (parse), download images and files (files), all of this (all)
mode = files
# crawler4j storage dir (where temporary files are stored)
storage_dir = crawler4j
# number of parallel crawling threads
number_of_crawlers = 10
# number of milliseconds between requests
politeness = 100
# maximum number of pages to fetch
max_pages = -1
# maximum crawling depth
max_depth = -1
# file with proxy list (ip addresses)
proxies_file = crawler/conf/proxies.txt
# after how many pages loaded through current proxy we need to change proxy (0 - never change proxy)
urls_per_proxy = 0
# file with seed urls and urls whose pages should be parsed and transformed with corresponding XSL templates 
# file format: 
# regex_url_mask <space> template <space> http://sample.url/to_test_regex
# or 
# seed_url <empty string>
# or
# url_to_be_processed_to_find_links_but_not_parsed <space> - <dash> <space> http://sample.url/to_test_regex
# # - comment symbol. Strings beginning with # are comments 
urls = crawler/conf/urls.txt
# XSL files directory (style files that are referenced in "urls" file)
styles_dir = crawler/conf/styles/
# the name of the directory where results are stored
result_dir = crawler/parse/result/
# the name of the resulting XML file
result_file = crawler/parse/result/result.xml
